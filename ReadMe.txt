    Данный файл содержит описание файлов в решении и инструкцию по запуску скрипта.
    Мое решение содержит 6 файлов:
        1. Readme.txt - данный файл;
        2. model_training_feature_mining_code.ipynb - юпитер ноутбук, содержащий основной код со сбором признаков и обучением модели;
        3. script_prognoz_result.ipynb - юпитер ноутбук, содержащий скрипт с прогнозированием и формированием файла submission.csv;
        4. submission.csv - решение, содержащее предсказание на тестовой выборке;
        5. test_filtered.csv - переработанная тестовая выборка, в которой реализованы новые признаки и на основе которой будет происходить прогнозированию;
        6. xgb_model.pkl - файл, в котором храниться модель XGBoost, обученная на нашей тренировочной выборке;

Инструкция по запуску скрипта script_prognoz_result.ipynb:
    1. Рекомендую запускать данный скрипт в Google Colab или Jupiter Notebook;
    2. Для запуска скрипта необходимы файлы test_filtered.csv и xgb_model.pkl;
    3. Скрипт реализован таким образом, что, чтобы он зароботал, необходимо разместить файлы, указанные во втором пункте, в той же директории, что и скрипт;
    4. После запуска скрипта, в директории, где находится сам скрипт, появится файл submission.csv, единтичный тому, что я прикрепил;

Файл script_prognoz_result.ipynb:
    Данный файлс помощью модели XGBoost создает результирующий файл submission.csv с прогнозированием нашей тестовой выборки, подготовленной для работы. Модель обучена в файле model_training_feature_mining_code.ipynb,
тестовые данные подготовлены в том же файле, что и модель. 

Файл model_training_feature_mining_code.ipynb:
    1 часть: Распоковывем наши файлы train.parquet и test.parquet и анализируем данные в датасетах (типы данных, формат данных, количество данных);
    2 часть: EDA. У нас нет никакого контекста данных, поэтому за признаки будем брать стандартные параметры. Столбик values содержит массив с результатами наблюдений,
поэтому вместо массива, будем смотреть на минимальное значение, максимальное, среднее значение, медиану, среднеквадратичное отклонение, количество измерений и модуль разницы между max и min.
Столбик dates разобьем на два признака, начало измерений и конец измерений (даты). Построим графики новых признаков и разделим на наши классы (label_0 и label_1).
По итогу, строим таблицу корреляции.
    3 часть: Предобработка данных. Мы выявили новые признаки, добавляем новые столбики с признаками в наши датасеты (train и test) и убираем столбики, с которыми мы не можем работать дальше (values и dates).
Также столбики с начальной и конечной датой измерений необходимо перевести в численный формат, так как модели плохо обрабатывают даты. Я перевожу в даты по правилу timestamp (в секунды).
Дальше я анализирую данные на наличие Nan. В тренировочной выборке 88 Nan и 22 Nan в тестовой выборке, все Nan находятся в столбиках, связанных с измерениями.
Nan я заполняю средними значениями по соответствующим данным.
    4 часть: Feature Importance. Я рассмотрел 5 классических моделей классификации (XGBoost, Logistic Regression, Random Forest Classify, Catboost, k-Nearest Neighbors). Обучаю каждую модель на нашей тренировочной выборке.
Для каждой модели рассматриваю значение целевой метрики Roc Auc. Я построил два графика, на первом показаны какие признаки и на сколько считают важными каждая из моделей (кроме KNN, так как для данной модели нельзя получить 
коэффициенты признаков), второй график нужен, чтобы наглядно показать, какая модель имеет лучший показатель Roc Auc. Отсюда получаем, что лучшее прогназирование дает нам модель XGBoost.
    5 часть: Подготовка файлов для прогнозирования. Сохраняем нашу подготовленную тестовую выборку в файл test_filtered.csv, а в файл xgb_model.pkl сохраняем нашу обученную модель.

Файл submission.csv:
    Содержит решение, соответствующее примеру решению. Два столбика: id, score.

Файл test_filtered.csv:
    Тестовая выборка, подготовленная к прогнозированию.

Файл xgb_model.pkl:
    Файл содержит обученную модель XGBoost.
